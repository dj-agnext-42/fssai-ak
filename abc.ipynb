{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Read the Excel file\n",
    "# df = pd.read_excel('Final_fssai.xlsx')\n",
    "\n",
    "# # Get all columns from 'Added Colouring Matter' to 'FSSR Limits Copper Sulphate'\n",
    "# all_columns = df.columns.tolist()\n",
    "# start_col = 'Commodity'\n",
    "# end_col = 'FSSR Limits Copper Sulphate (Copper determined as elemental copper'\n",
    "\n",
    "# # Find the indices of start and end columns\n",
    "# start_idx = all_columns.index(start_col)\n",
    "# end_idx = all_columns.index(end_col)\n",
    "\n",
    "# # Create list of columns we want to keep\n",
    "# selected_columns = ['Order ID'] + all_columns[start_idx:end_idx+1]\n",
    "\n",
    "# # Create a new Excel writer object\n",
    "# with pd.ExcelWriter('Final_fssai_commodity_wise.xlsx', engine='openpyxl') as writer:\n",
    "#     # Iterate through each unique commodity\n",
    "#     for commodity in df['Commodity'].unique():\n",
    "#         # Filter data for the current commodity\n",
    "#         commodity_data = df[df['Commodity'] == commodity][selected_columns]\n",
    "        \n",
    "#         # Clean sheet name (remove special characters that are not allowed in Excel sheet names)\n",
    "#         sheet_name = commodity.replace('/', '_').replace('\\\\', '_')[:31]  # Excel has 31 char limit for sheet names\n",
    "        \n",
    "#         # Write the filtered data to a new sheet\n",
    "#         commodity_data.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "# print(\"Process completed! Check 'commodity_wise_data.xlsx' for results.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Read the existing Excel file with all sheets\n",
    "# excel_file = pd.ExcelFile('commodity_wise_data_reorganized3.xlsx')\n",
    "\n",
    "# # Create a new Excel writer object\n",
    "# with pd.ExcelWriter('commodity_wise_data_cleaned_final.xlsx', engine='openpyxl') as writer:\n",
    "#     # Process each sheet\n",
    "#     for sheet_name in excel_file.sheet_names:\n",
    "#         # Read the sheet\n",
    "#         df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "        \n",
    "#         # Drop completely empty columns\n",
    "#         df_cleaned = df.dropna(axis=1, how='all')\n",
    "        \n",
    "#         # Write to new Excel file\n",
    "#         df_cleaned.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "# print(\"Process completed! Check 'commodity_wise_data_cleaned.xlsx' for results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# def get_parameter_columns(df, param_name):\n",
    "#     \"\"\"Get all three columns associated with a parameter\"\"\"\n",
    "#     return [\n",
    "#         col for col in df.columns \n",
    "#         if col == param_name \n",
    "#         or col == f\"{param_name} Compliance\"\n",
    "#         or col.startswith(f\"FSSR Limits {param_name}\")\n",
    "#     ]\n",
    "\n",
    "# def is_safe_unsafe_param(df, param_name):\n",
    "#     \"\"\"Check if parameter has Safe/Unsafe compliance\"\"\"\n",
    "#     compliance_col = f\"{param_name} Compliance\"\n",
    "#     if compliance_col in df.columns:\n",
    "#         unique_values = df[compliance_col].dropna().unique()\n",
    "#         return any(val in ['Safe', 'Unsafe'] for val in unique_values)\n",
    "#     return False\n",
    "\n",
    "# def get_base_parameters(df):\n",
    "#     \"\"\"Get list of base parameter names (without Compliance or FSSR Limits prefix)\"\"\"\n",
    "#     # Get columns that don't have 'Compliance' or 'FSSR Limits' in their names\n",
    "#     base_params = [col for col in df.columns \n",
    "#                    if 'Compliance' not in col \n",
    "#                    and 'FSSR Limits' not in col\n",
    "#                    and col != 'Order ID']\n",
    "#     return base_params\n",
    "\n",
    "# def reorganize_sheet(df):\n",
    "#     \"\"\"Reorganize columns based on compliance type\"\"\"\n",
    "#     # Keep Order ID as first column\n",
    "#     new_columns = ['Order ID']\n",
    "    \n",
    "#     # Get base parameter names\n",
    "#     base_params = get_base_parameters(df)\n",
    "    \n",
    "#     # Separate parameters into Safe/Unsafe and Standard/Sub-Standard\n",
    "#     safe_unsafe_params = []\n",
    "#     standard_substandard_params = []\n",
    "    \n",
    "#     for param in base_params:\n",
    "#         if is_safe_unsafe_param(df, param):\n",
    "#             safe_unsafe_params.append(param)\n",
    "#         else:\n",
    "#             standard_substandard_params.append(param)\n",
    "    \n",
    "#     # Add Safe/Unsafe parameter columns\n",
    "#     for param in safe_unsafe_params:\n",
    "#         new_columns.extend(get_parameter_columns(df, param))\n",
    "    \n",
    "#     # Add Standard/Sub-Standard parameter columns\n",
    "#     for param in standard_substandard_params:\n",
    "#         new_columns.extend(get_parameter_columns(df, param))\n",
    "    \n",
    "#     # Reorder the dataframe columns\n",
    "#     return df[new_columns]\n",
    "\n",
    "# # Read the existing Excel file\n",
    "# excel_file = pd.ExcelFile('commodity_wise_data_cleaned2.xlsx')\n",
    "\n",
    "# # Process each sheet and save to new file\n",
    "# with pd.ExcelWriter('commodity_wise_data_reorganized2.xlsx', engine='openpyxl') as writer:\n",
    "#     for sheet_name in excel_file.sheet_names:\n",
    "#         # Read the sheet\n",
    "#         df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "        \n",
    "#         # Reorganize columns\n",
    "#         df_reorganized = reorganize_sheet(df)\n",
    "        \n",
    "#         # Write to new Excel file\n",
    "#         df_reorganized.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "# print(\"Process completed! Check 'commodity_wise_data_reorganized.xlsx' for results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# def get_parameter_columns(df, param_name):\n",
    "#     \"\"\"Get all three columns associated with a parameter\"\"\"\n",
    "#     return [\n",
    "#         col for col in df.columns \n",
    "#         if col == param_name \n",
    "#         or col == f\"{param_name} Compliance\"\n",
    "#         or col.startswith(f\"FSSR Limits {param_name}\")\n",
    "#     ]\n",
    "\n",
    "# def is_safe_unsafe_param(df, param_name):\n",
    "#     \"\"\"Check if parameter has Safe/Unsafe compliance\"\"\"\n",
    "#     compliance_col = f\"{param_name} Compliance\"\n",
    "#     if compliance_col in df.columns:\n",
    "#         unique_values = df[compliance_col].dropna().unique()\n",
    "#         return any(val in ['Safe', 'Unsafe'] for val in unique_values)\n",
    "#     return False\n",
    "\n",
    "# def get_base_parameters(df):\n",
    "#     \"\"\"Get list of base parameter names (without Compliance or FSSR Limits prefix)\"\"\"\n",
    "#     # Get columns that don't have 'Compliance' or 'FSSR Limits' in their names\n",
    "#     # and are not in the initial columns list\n",
    "#     initial_cols = ['Order ID', 'Commodity', 'Count of Passed Residue Analysis']\n",
    "#     base_params = [col for col in df.columns \n",
    "#                    if 'Compliance' not in col \n",
    "#                    and 'FSSR Limits' not in col\n",
    "#                    and col not in initial_cols]\n",
    "#     return base_params\n",
    "\n",
    "# def get_initial_columns(df):\n",
    "#     \"\"\"Get all columns from 'Order ID' to 'Count of Passed Residue Analysis'\"\"\"\n",
    "#     all_cols = df.columns.tolist()\n",
    "#     start_idx = all_cols.index('Order ID')\n",
    "#     end_idx = all_cols.index('Count of Passed Residue Analysis')\n",
    "#     return all_cols[start_idx:end_idx + 1]\n",
    "\n",
    "# def reorganize_sheet(df):\n",
    "#     \"\"\"Reorganize columns based on compliance type\"\"\"\n",
    "#     # Get initial columns (from Order ID to Count of Passed Residue Analysis)\n",
    "#     new_columns = get_initial_columns(df)\n",
    "    \n",
    "#     # Get base parameter names\n",
    "#     base_params = get_base_parameters(df)\n",
    "    \n",
    "#     # Separate parameters into Safe/Unsafe and Standard/Sub-Standard\n",
    "#     safe_unsafe_params = []\n",
    "#     standard_substandard_params = []\n",
    "    \n",
    "#     for param in base_params:\n",
    "#         if is_safe_unsafe_param(df, param):\n",
    "#             safe_unsafe_params.append(param)\n",
    "#         else:\n",
    "#             standard_substandard_params.append(param)\n",
    "    \n",
    "#     # Add Safe/Unsafe parameter columns\n",
    "#     for param in safe_unsafe_params:\n",
    "#         new_columns.extend(get_parameter_columns(df, param))\n",
    "    \n",
    "#     # Add Standard/Sub-Standard parameter columns\n",
    "#     for param in standard_substandard_params:\n",
    "#         new_columns.extend(get_parameter_columns(df, param))\n",
    "    \n",
    "#     # Reorder the dataframe columns\n",
    "#     return df[new_columns]\n",
    "\n",
    "# # Read the existing Excel file\n",
    "# excel_file = pd.ExcelFile('commodity_wise_data_cleaned2.xlsx')\n",
    "\n",
    "# # Process each sheet and save to new file\n",
    "# with pd.ExcelWriter('commodity_wise_data_reorganized2.xlsx', engine='openpyxl') as writer:\n",
    "#     for sheet_name in excel_file.sheet_names:\n",
    "#         # Read the sheet\n",
    "#         df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "        \n",
    "#         # Reorganize columns\n",
    "#         df_reorganized = reorganize_sheet(df)\n",
    "        \n",
    "#         # Write to new Excel file\n",
    "#         df_reorganized.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "# print(\"Process completed! Check 'commodity_wise_data_reorganized.xlsx' for results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# def get_parameter_columns(df, param_name):\n",
    "#     \"\"\"Get all three columns associated with a parameter\"\"\"\n",
    "#     return [\n",
    "#         col for col in df.columns \n",
    "#         if col == param_name \n",
    "#         or col == f\"{param_name} Compliance\"\n",
    "#         or col.startswith(f\"FSSR Limits {param_name}\")\n",
    "#     ]\n",
    "\n",
    "# def is_safe_unsafe_param(df, param_name):\n",
    "#     \"\"\"Check if parameter has Safe/Unsafe compliance\"\"\"\n",
    "#     compliance_col = f\"{param_name} Compliance\"\n",
    "#     if compliance_col in df.columns:\n",
    "#         unique_values = df[compliance_col].dropna().unique()\n",
    "#         return any(val in ['Safe', 'Unsafe'] for val in unique_values)\n",
    "#     return False\n",
    "\n",
    "# def get_initial_columns(df):\n",
    "#     \"\"\"Get all columns from 'Order ID' to 'Count of Passed Residue Analysis'\"\"\"\n",
    "#     all_cols = df.columns.tolist()\n",
    "#     start_idx = all_cols.index('Order ID')\n",
    "#     end_idx = all_cols.index('Count of Passed Residue Analysis')\n",
    "#     return all_cols[start_idx:end_idx + 1]\n",
    "\n",
    "# def get_base_parameters(df, initial_cols):\n",
    "#     \"\"\"Get list of base parameter names (without Compliance or FSSR Limits prefix)\"\"\"\n",
    "#     # Get columns that don't have 'Compliance' or 'FSSR Limits' in their names\n",
    "#     # and are not in the initial columns list\n",
    "#     base_params = [col for col in df.columns \n",
    "#                    if 'Compliance' not in col \n",
    "#                    and 'FSSR Limits' not in col\n",
    "#                    and col not in initial_cols]\n",
    "#     return base_params\n",
    "\n",
    "# def reorganize_sheet(df):\n",
    "#     \"\"\"Reorganize columns based on compliance type\"\"\"\n",
    "#     # Get initial columns (from Order ID to Count of Passed Residue Analysis)\n",
    "#     initial_columns = get_initial_columns(df)\n",
    "#     new_columns = initial_columns\n",
    "    \n",
    "#     # Get base parameter names (excluding initial columns)\n",
    "#     base_params = get_base_parameters(df, initial_columns)\n",
    "    \n",
    "#     # Separate parameters into Safe/Unsafe and Standard/Sub-Standard\n",
    "#     safe_unsafe_params = []\n",
    "#     standard_substandard_params = []\n",
    "    \n",
    "#     for param in base_params:\n",
    "#         if is_safe_unsafe_param(df, param):\n",
    "#             safe_unsafe_params.append(param)\n",
    "#         else:\n",
    "#             standard_substandard_params.append(param)\n",
    "    \n",
    "#     # Add Safe/Unsafe parameter columns\n",
    "#     for param in safe_unsafe_params:\n",
    "#         new_columns.extend(get_parameter_columns(df, param))\n",
    "    \n",
    "#     # Add Standard/Sub-Standard parameter columns\n",
    "#     for param in standard_substandard_params:\n",
    "#         new_columns.extend(get_parameter_columns(df, param))\n",
    "    \n",
    "#     # Reorder the dataframe columns\n",
    "#     return df[new_columns]\n",
    "\n",
    "# # First, let's print out the initial columns to verify\n",
    "# sample_df = pd.read_excel('Final_fssai_commodity_wise.xlsx', sheet_name=0)\n",
    "# initial_cols = get_initial_columns(sample_df)\n",
    "# print(\"Initial columns that will be preserved in order:\")\n",
    "# print(initial_cols)\n",
    "# print(\"\\n\")\n",
    "\n",
    "# # Process each sheet and save to new file\n",
    "# excel_file = pd.ExcelFile('Final_fssai_commodity_wise.xlsx')\n",
    "\n",
    "# with pd.ExcelWriter('Final_fssai_commodity_wise_rearanged.xlsx.xlsx', engine='openpyxl') as writer:\n",
    "#     for sheet_name in excel_file.sheet_names:\n",
    "#         # Read the sheet\n",
    "#         df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "        \n",
    "#         # Reorganize columns\n",
    "#         df_reorganized = reorganize_sheet(df)\n",
    "        \n",
    "#         # Write to new Excel file\n",
    "#         df_reorganized.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "# print(\"Process completed! Check 'commodity_wise_data_reorganized2.xlsx' for results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# def get_parameter_columns(df, param_name):\n",
    "#     \"\"\"Get all three columns associated with a parameter\"\"\"\n",
    "#     return [\n",
    "#         col for col in df.columns \n",
    "#         if col == param_name \n",
    "#         or col == f\"{param_name} Compliance\"\n",
    "#         or col.startswith(f\"FSSR Limits {param_name}\")\n",
    "#     ]\n",
    "\n",
    "# def is_column_empty(df, column):\n",
    "#     \"\"\"Check if a column is completely empty (all NaN or empty strings)\"\"\"\n",
    "#     if column not in df.columns:\n",
    "#         return True\n",
    "#     return df[column].isna().all() or (df[column] == '').all()\n",
    "\n",
    "# def is_parameter_complete(df, param_name):\n",
    "#     \"\"\"Check if all three columns for a parameter are present and not empty\"\"\"\n",
    "#     base_col = param_name\n",
    "#     compliance_col = f\"{param_name} Compliance\"\n",
    "#     fssr_col = f\"FSSR Limits {param_name}\"\n",
    "    \n",
    "#     # Check if any column is empty\n",
    "#     if is_column_empty(df, base_col) or \\\n",
    "#        is_column_empty(df, compliance_col) or \\\n",
    "#        is_column_empty(df, fssr_col):\n",
    "#         return False\n",
    "#     return True\n",
    "\n",
    "# def get_initial_columns(df):\n",
    "#     \"\"\"Get all columns from 'Order ID' to 'Count of Passed Residue Analysis'\"\"\"\n",
    "#     all_cols = df.columns.tolist()\n",
    "#     start_idx = all_cols.index('Order ID')\n",
    "#     end_idx = all_cols.index('Count of Passed Residue Analysis')\n",
    "#     return all_cols[start_idx:end_idx + 1]\n",
    "\n",
    "# def get_base_parameters(df, initial_cols):\n",
    "#     \"\"\"Get list of base parameter names (without Compliance or FSSR Limits prefix)\"\"\"\n",
    "#     base_params = [col for col in df.columns \n",
    "#                    if 'Compliance' not in col \n",
    "#                    and 'FSSR Limits' not in col\n",
    "#                    and col not in initial_cols]\n",
    "#     return base_params\n",
    "\n",
    "# def is_safe_unsafe_param(df, param_name):\n",
    "#     \"\"\"Check if parameter has Safe/Unsafe compliance\"\"\"\n",
    "#     compliance_col = f\"{param_name} Compliance\"\n",
    "#     if compliance_col in df.columns:\n",
    "#         unique_values = df[compliance_col].dropna().unique()\n",
    "#         return any(val in ['Safe', 'Unsafe'] for val in unique_values)\n",
    "#     return False\n",
    "\n",
    "# def reorganize_sheet(df):\n",
    "#     \"\"\"Reorganize columns and remove incomplete parameter sets\"\"\"\n",
    "#     # Get initial columns\n",
    "#     initial_columns = get_initial_columns(df)\n",
    "#     new_columns = initial_columns.copy()\n",
    "    \n",
    "#     # Get base parameter names\n",
    "#     base_params = get_base_parameters(df, initial_columns)\n",
    "    \n",
    "#     # Filter out incomplete parameter sets and separate into Safe/Unsafe and Standard/Sub-Standard\n",
    "#     safe_unsafe_params = []\n",
    "#     standard_substandard_params = []\n",
    "    \n",
    "#     for param in base_params:\n",
    "#         if is_parameter_complete(df, param):\n",
    "#             if is_safe_unsafe_param(df, param):\n",
    "#                 safe_unsafe_params.append(param)\n",
    "#             else:\n",
    "#                 standard_substandard_params.append(param)\n",
    "    \n",
    "#     # Add Safe/Unsafe parameter columns\n",
    "#     for param in safe_unsafe_params:\n",
    "#         new_columns.extend(get_parameter_columns(df, param))\n",
    "    \n",
    "#     # Add Standard/Sub-Standard parameter columns\n",
    "#     for param in standard_substandard_params:\n",
    "#         new_columns.extend(get_parameter_columns(df, param))\n",
    "    \n",
    "#     # Return dataframe with only the selected columns\n",
    "#     return df[new_columns]\n",
    "\n",
    "# # Process the Excel file\n",
    "# excel_file = pd.ExcelFile('Final_fssai_commodity_wise_rearanged.xlsx.xlsx')\n",
    "\n",
    "# # Create a new Excel writer\n",
    "# with pd.ExcelWriter('Final_fssai_commodity_wise_rearanged_drop.xlsx', engine='openpyxl') as writer:\n",
    "#     for sheet_name in excel_file.sheet_names:\n",
    "#         print(f\"\\nProcessing sheet: {sheet_name}\")\n",
    "        \n",
    "#         # Read the sheet\n",
    "#         df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "#         original_cols = len(df.columns)\n",
    "        \n",
    "#         # Reorganize and clean columns\n",
    "#         df_cleaned = reorganize_sheet(df)\n",
    "#         final_cols = len(df_cleaned.columns)\n",
    "        \n",
    "#         print(f\"Columns removed: {original_cols - final_cols}\")\n",
    "        \n",
    "#         # Write to new Excel file\n",
    "#         df_cleaned.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "# print(\"\\nProcess completed! Check 'commodity_wise_data_complete_params.xlsx' for results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# def get_parameter_columns(df, param_name):\n",
    "#     \"\"\"Get all three columns associated with a parameter\"\"\"\n",
    "#     return [\n",
    "#         col for col in df.columns \n",
    "#         if col == param_name \n",
    "#         or col == f\"{param_name} Compliance\"\n",
    "#         or col.startswith(f\"FSSR Limits {param_name}\")\n",
    "#     ]\n",
    "\n",
    "# def is_param_empty(df, param_name):\n",
    "#     \"\"\"Check if the parameter column is completely empty\"\"\"\n",
    "#     if param_name not in df.columns:\n",
    "#         return True\n",
    "#     return df[param_name].isna().all() or (df[param_name] == '').all()\n",
    "\n",
    "# def get_initial_columns(df):\n",
    "#     \"\"\"Get all columns from 'Order ID' to 'Count of Passed Residue Analysis'\"\"\"\n",
    "#     all_cols = df.columns.tolist()\n",
    "#     start_idx = all_cols.index('Order ID')\n",
    "#     end_idx = all_cols.index('Count of Passed Residue Analysis')\n",
    "#     return all_cols[start_idx:end_idx + 1]\n",
    "\n",
    "# def get_base_parameters(df, initial_cols):\n",
    "#     \"\"\"Get list of base parameter names (without Compliance or FSSR Limits prefix)\"\"\"\n",
    "#     base_params = [col for col in df.columns \n",
    "#                    if 'Compliance' not in col \n",
    "#                    and 'FSSR Limits' not in col\n",
    "#                    and col not in initial_cols]\n",
    "#     return base_params\n",
    "\n",
    "# def clean_sheet(df):\n",
    "#     \"\"\"Remove parameter groups where parameter column is empty\"\"\"\n",
    "#     # Get initial columns\n",
    "#     initial_columns = get_initial_columns(df)\n",
    "#     columns_to_keep = initial_columns.copy()\n",
    "    \n",
    "#     # Get base parameter names\n",
    "#     base_params = get_base_parameters(df, initial_columns)\n",
    "    \n",
    "#     # For each parameter, check if its parameter column is empty\n",
    "#     for param in base_params:\n",
    "#         if not is_param_empty(df, param):\n",
    "#             # If parameter column is not empty, keep all three associated columns\n",
    "#             columns_to_keep.extend(get_parameter_columns(df, param))\n",
    "    \n",
    "#     # Return dataframe with only the selected columns\n",
    "#     return df[columns_to_keep]\n",
    "\n",
    "# # Read the existing Excel file\n",
    "# excel_file = pd.ExcelFile('Final_fssai_commodity_wise_rearanged.xlsx.xlsx')\n",
    "\n",
    "# # Process each sheet and save to new file\n",
    "# with pd.ExcelWriter('commodity_wise_data_cleaned_final2.xlsx', engine='openpyxl') as writer:\n",
    "#     for sheet_name in excel_file.sheet_names:\n",
    "#         print(f\"\\nProcessing sheet: {sheet_name}\")\n",
    "        \n",
    "#         # Read the sheet\n",
    "#         df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "#         original_cols = len(df.columns)\n",
    "        \n",
    "#         # Clean the sheet by removing parameter groups with empty parameter columns\n",
    "#         df_cleaned = clean_sheet(df)\n",
    "#         final_cols = len(df_cleaned.columns)\n",
    "        \n",
    "#         print(f\"Columns removed: {original_cols - final_cols}\")\n",
    "        \n",
    "#         # Write to new Excel file\n",
    "#         df_cleaned.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "# print(\"\\nProcess completed! Check 'commodity_wise_data_cleaned_final2.xlsx' for results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# def export_hyperlinks_to_excel(df, column_name, excel_path):\n",
    "#     \"\"\"\n",
    "#     Converts URLs in the specified column to clickable hyperlinks and exports the DataFrame to an Excel file.\n",
    "#     Parameters:\n",
    "#         df (pd.DataFrame): The DataFrame containing the column with URLs.\n",
    "#         column_name (str): The name of the column containing URLs.\n",
    "#         excel_path (str): The output file path for the Excel file.\n",
    "#     Returns:\n",
    "#         None\n",
    "#     \"\"\"\n",
    "#     # Apply HYPERLINK formula to the specified column\n",
    "#     df[column_name] = df[column_name].apply(lambda x: f'=HYPERLINK(\"{x}\", \"{x}\")')\n",
    "#     # Export to Excel using openpyxl (or xlsxwriter if needed)\n",
    "#     with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
    "#         df.to_excel(writer, sheet_name='Sheet1', index=False)\n",
    "#     print(f\"Excel file saved as {excel_path}\")\n",
    "# # Example usage:\n",
    "# df = pd.DataFrame({'Website Links': ['https://www.google.com', 'https://www.github.com']})\n",
    "# export_hyperlinks_to_excel(df, 'Website Links', 'output.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# def add_hyperlink(df, column_names):\n",
    "    \n",
    "#     for column in column_names:\n",
    "#         df[column] = df[column].apply(lambda x: f'=HYPERLINK(\"{x}\", \"{x}\")' if pd.notna(x) else x)\n",
    "#     return df\n",
    "\n",
    "# def date_change(df2_clean):\n",
    "#     # df2_clean[['date_1', 'date_2', 'date_3']] = df2_clean[['date_1', 'date_2', 'date_3']].fillna('Not Applicable')\n",
    "# # Create masks for different variants\n",
    "#     loose_mask = df2_clean['Variant'] == 'Loose'\n",
    "#     other_mask = df2_clean['Variant'].isin(['Normal', 'Organic'])\n",
    "#    # Replace values for date_2 and date_3 based on conditions\n",
    "#     for col in ['Date of sample collection', 'Expiry date','Manufacturing date']:\n",
    "#        df2_clean.loc[loose_mask, col] = df2_clean.loc[loose_mask, col].fillna('Not Applicable')\n",
    "#        df2_clean.loc[other_mask, col] = df2_clean.loc[other_mask, col].fillna('Not Available')\n",
    "#     return df2_clean\n",
    "\n",
    "\n",
    "# column_names = ['Report Link','Picture of the shop','General Packaging Link','Manufacturing Details Photo Link']\n",
    "# df=pd.read_excel('fssai.xlsx',sheet_name='MASTER DATA')\n",
    "\n",
    "# df_new=add_hyperlink(df,column_names)\n",
    "\n",
    "# df_new=date_change(df_new)\n",
    "\n",
    "# df_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_new.to_excel('Final_fssai.xlsx', sheet_name='Sheet1', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# Displaying a warning message \n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Using cached numpy-2.2.3-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\dev_playground\\13-march\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in d:\\dev_playground\\13-march\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "Using cached numpy-2.2.3-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "Using cached pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "Using cached tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-2.2.3 pandas-2.2.3 pytz-2025.1 tzdata-2025.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Worksheet named 'MASTER DATA' not found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m column_names = [\u001b[33m'\u001b[39m\u001b[33mReport Link\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mPicture of the shop\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mGeneral Packaging Link\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mManufacturing Details Photo Link\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mLab Report Link\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df=\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mFSSAI.xlsx\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mMASTER DATA\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#, skiprows=1)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Dev_Playground\\13-march\\.venv\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:508\u001b[39m, in \u001b[36mread_excel\u001b[39m\u001b[34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[39m\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    503\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mEngine should not be specified when passing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    505\u001b[39m     )\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m508\u001b[39m     data = \u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m        \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[43m=\u001b[49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[43m        \u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m        \u001b[49m\u001b[43mna_filter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_filter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m        \u001b[49m\u001b[43mthousands\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    534\u001b[39m     \u001b[38;5;66;03m# make sure to close opened file handles\u001b[39;00m\n\u001b[32m    535\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m should_close:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Dev_Playground\\13-march\\.venv\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1616\u001b[39m, in \u001b[36mExcelFile.parse\u001b[39m\u001b[34m(self, sheet_name, header, names, index_col, usecols, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, date_format, thousands, comment, skipfooter, dtype_backend, **kwds)\u001b[39m\n\u001b[32m   1576\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse\u001b[39m(\n\u001b[32m   1577\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1578\u001b[39m     sheet_name: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28mint\u001b[39m | \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m] | \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[32m0\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1596\u001b[39m     **kwds,\n\u001b[32m   1597\u001b[39m ) -> DataFrame | \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, DataFrame] | \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mint\u001b[39m, DataFrame]:\n\u001b[32m   1598\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1599\u001b[39m \u001b[33;03m    Parse specified sheet(s) into a DataFrame.\u001b[39;00m\n\u001b[32m   1600\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1614\u001b[39m \u001b[33;03m    >>> file.parse()  # doctest: +SKIP\u001b[39;00m\n\u001b[32m   1615\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1617\u001b[39m \u001b[43m        \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1618\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1619\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1620\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1621\u001b[39m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[43m=\u001b[49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1622\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1623\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1624\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1625\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1626\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1627\u001b[39m \u001b[43m        \u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1628\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1630\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1631\u001b[39m \u001b[43m        \u001b[49m\u001b[43mthousands\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1632\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1633\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1634\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1635\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1636\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Dev_Playground\\13-march\\.venv\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:773\u001b[39m, in \u001b[36mBaseExcelReader.parse\u001b[39m\u001b[34m(self, sheet_name, header, names, index_col, usecols, dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, dtype_backend, **kwds)\u001b[39m\n\u001b[32m    770\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mReading sheet \u001b[39m\u001b[38;5;132;01m{\u001b[39;00masheetname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(asheetname, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m773\u001b[39m     sheet = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_sheet_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43masheetname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# assume an integer if not a string\u001b[39;00m\n\u001b[32m    775\u001b[39m     sheet = \u001b[38;5;28mself\u001b[39m.get_sheet_by_index(asheetname)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Dev_Playground\\13-march\\.venv\\Lib\\site-packages\\pandas\\io\\excel\\_openpyxl.py:582\u001b[39m, in \u001b[36mOpenpyxlReader.get_sheet_by_name\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    581\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_sheet_by_name\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m582\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraise_if_bad_sheet_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    583\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.book[name]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Dev_Playground\\13-march\\.venv\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:624\u001b[39m, in \u001b[36mBaseExcelReader.raise_if_bad_sheet_by_name\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mraise_if_bad_sheet_by_name\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.sheet_names:\n\u001b[32m--> \u001b[39m\u001b[32m624\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWorksheet named \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m not found\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Worksheet named 'MASTER DATA' not found"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "column_names = ['Report Link','Picture of the shop','General Packaging Link','Manufacturing Details Photo Link', 'Lab Report Link']\n",
    "df=pd.read_excel('FSSAI.xlsx', sheet_name='MASTER DATA') #, skiprows=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_hyperlink(df, column_names):\n",
    "    \n",
    "    for column in column_names:\n",
    "        df[column] = df[column].apply(lambda x: f'=HYPERLINK(\"{x}\", \"{x}\")')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Order ID', 'Commodity', 'Type', 'Variant', 'Sector', 'Status', 'State',\n",
       "       'District of sample collection', 'Date of sample collection',\n",
       "       'The name of the shop',\n",
       "       ...\n",
       "       'Morphologically extraneous matter including foreign starch',\n",
       "       'Morphologically extraneous matter including foreign starch Compliance',\n",
       "       'FSSR Limits Morphologically extraneous matter including foreign starch',\n",
       "       'Colouring power expressed as curcuminoid content on dry basis, %',\n",
       "       'Colouring power expressed as curcuminoid content on dry basis, % Compliance',\n",
       "       'FSSR Limits Colouring power expressed as curcuminoid content on dry basis, %',\n",
       "       'Copper Sulphate (Copper determined as elemental copper',\n",
       "       'Copper Sulphate (Copper determined as elemental copper Compliance',\n",
       "       'FSSR Limits Copper Sulphate (Copper determined as elemental copper',\n",
       "       'Unnamed: 205'],\n",
       "      dtype='object', length=569)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Commodity</th>\n",
       "      <th>Type</th>\n",
       "      <th>Variant</th>\n",
       "      <th>Sector</th>\n",
       "      <th>Status</th>\n",
       "      <th>State</th>\n",
       "      <th>District of sample collection</th>\n",
       "      <th>Date of sample collection</th>\n",
       "      <th>The name of the shop</th>\n",
       "      <th>...</th>\n",
       "      <th>Morphologically extraneous matter including foreign starch</th>\n",
       "      <th>Morphologically extraneous matter including foreign starch Compliance</th>\n",
       "      <th>FSSR Limits Morphologically extraneous matter including foreign starch</th>\n",
       "      <th>Colouring power expressed as curcuminoid content on dry basis, %</th>\n",
       "      <th>Colouring power expressed as curcuminoid content on dry basis, % Compliance</th>\n",
       "      <th>FSSR Limits Colouring power expressed as curcuminoid content on dry basis, %</th>\n",
       "      <th>Copper Sulphate (Copper determined as elemental copper</th>\n",
       "      <th>Copper Sulphate (Copper determined as elemental copper Compliance</th>\n",
       "      <th>FSSR Limits Copper Sulphate (Copper determined as elemental copper</th>\n",
       "      <th>Unnamed: 205</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FS-TS-NI-CORIANDER_WHOLE-929255</td>\n",
       "      <td>Coriander Whole</td>\n",
       "      <td>Whole</td>\n",
       "      <td>Organic</td>\n",
       "      <td>Organized</td>\n",
       "      <td>Report Uploaded</td>\n",
       "      <td>Telangana</td>\n",
       "      <td>Nirmal</td>\n",
       "      <td>13-11-2024</td>\n",
       "      <td>Reliance smart</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Max 0.005 mg/kg</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FS-MH-AH-TURMERIC_POWDER-066985</td>\n",
       "      <td>Turmeric Powder</td>\n",
       "      <td>Powder</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Organized</td>\n",
       "      <td>Report Uploaded</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>Ahmadnagar</td>\n",
       "      <td>19-11-2024</td>\n",
       "      <td>Navkar super market</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Standard</td>\n",
       "      <td>Max 0.0 %</td>\n",
       "      <td>5.393</td>\n",
       "      <td>Standard</td>\n",
       "      <td>Min 2.0 %</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Max 0.1 mg/kg</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FS-TS-HY-CHILLI_POWDER-735915</td>\n",
       "      <td>Chilli Powder</td>\n",
       "      <td>Powder</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Organized</td>\n",
       "      <td>Under Testing</td>\n",
       "      <td>Telangana</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>13-11-2024</td>\n",
       "      <td>R k supermarket</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Max 0.1 mg/kg</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FS-TS-NI-CHILLI_POWDER-447745</td>\n",
       "      <td>Chilli Powder</td>\n",
       "      <td>Powder</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Organized</td>\n",
       "      <td>Report Uploaded</td>\n",
       "      <td>Telangana</td>\n",
       "      <td>Nizamabad</td>\n",
       "      <td>13-11-2024</td>\n",
       "      <td>New Aq kirana</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Max 0.1 mg/kg</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FS-TS-NI-CUMIN_POWDER-677737</td>\n",
       "      <td>Cumin Powder</td>\n",
       "      <td>Powder</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Organized</td>\n",
       "      <td>Report Uploaded</td>\n",
       "      <td>Telangana</td>\n",
       "      <td>Nizamabad</td>\n",
       "      <td>13-11-2024</td>\n",
       "      <td>Vasavi super market</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Max 0.1 mg/kg</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 569 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Order ID        Commodity    Type  Variant  \\\n",
       "0  FS-TS-NI-CORIANDER_WHOLE-929255  Coriander Whole   Whole  Organic   \n",
       "1  FS-MH-AH-TURMERIC_POWDER-066985  Turmeric Powder  Powder   Normal   \n",
       "2    FS-TS-HY-CHILLI_POWDER-735915    Chilli Powder  Powder   Normal   \n",
       "3    FS-TS-NI-CHILLI_POWDER-447745    Chilli Powder  Powder   Normal   \n",
       "4     FS-TS-NI-CUMIN_POWDER-677737     Cumin Powder  Powder   Normal   \n",
       "\n",
       "      Sector           Status        State District of sample collection  \\\n",
       "0  Organized  Report Uploaded    Telangana                        Nirmal   \n",
       "1  Organized  Report Uploaded  Maharashtra                    Ahmadnagar   \n",
       "2  Organized    Under Testing    Telangana                     Hyderabad   \n",
       "3  Organized  Report Uploaded    Telangana                     Nizamabad   \n",
       "4  Organized  Report Uploaded    Telangana                     Nizamabad   \n",
       "\n",
       "  Date of sample collection The name of the shop  ...  \\\n",
       "0                13-11-2024       Reliance smart  ...   \n",
       "1                19-11-2024  Navkar super market  ...   \n",
       "2                13-11-2024      R k supermarket  ...   \n",
       "3                13-11-2024        New Aq kirana  ...   \n",
       "4                13-11-2024  Vasavi super market  ...   \n",
       "\n",
       "  Morphologically extraneous matter including foreign starch  \\\n",
       "0                                                NaN           \n",
       "1                                                0.0           \n",
       "2                                                NaN           \n",
       "3                                                NaN           \n",
       "4                                                NaN           \n",
       "\n",
       "  Morphologically extraneous matter including foreign starch Compliance  \\\n",
       "0                                                NaN                      \n",
       "1                                           Standard                      \n",
       "2                                                NaN                      \n",
       "3                                                NaN                      \n",
       "4                                                NaN                      \n",
       "\n",
       "   FSSR Limits Morphologically extraneous matter including foreign starch  \\\n",
       "0                                                NaN                        \n",
       "1                                          Max 0.0 %                        \n",
       "2                                                NaN                        \n",
       "3                                                NaN                        \n",
       "4                                                NaN                        \n",
       "\n",
       "  Colouring power expressed as curcuminoid content on dry basis, %  \\\n",
       "0                                                NaN                 \n",
       "1                                              5.393                 \n",
       "2                                                NaN                 \n",
       "3                                                NaN                 \n",
       "4                                                NaN                 \n",
       "\n",
       "  Colouring power expressed as curcuminoid content on dry basis, % Compliance  \\\n",
       "0                                                NaN                            \n",
       "1                                           Standard                            \n",
       "2                                                NaN                            \n",
       "3                                                NaN                            \n",
       "4                                                NaN                            \n",
       "\n",
       "  FSSR Limits Colouring power expressed as curcuminoid content on dry basis, %  \\\n",
       "0                                                NaN                             \n",
       "1                                          Min 2.0 %                             \n",
       "2                                                NaN                             \n",
       "3                                                NaN                             \n",
       "4                                                NaN                             \n",
       "\n",
       "  Copper Sulphate (Copper determined as elemental copper  \\\n",
       "0                                                NaN       \n",
       "1                                                NaN       \n",
       "2                                                NaN       \n",
       "3                                                NaN       \n",
       "4                                                NaN       \n",
       "\n",
       "  Copper Sulphate (Copper determined as elemental copper Compliance  \\\n",
       "0                                                NaN                  \n",
       "1                                                NaN                  \n",
       "2                                                NaN                  \n",
       "3                                                NaN                  \n",
       "4                                                NaN                  \n",
       "\n",
       "  FSSR Limits Copper Sulphate (Copper determined as elemental copper  \\\n",
       "0                                    Max 0.005 mg/kg                   \n",
       "1                                      Max 0.1 mg/kg                   \n",
       "2                                      Max 0.1 mg/kg                   \n",
       "3                                      Max 0.1 mg/kg                   \n",
       "4                                      Max 0.1 mg/kg                   \n",
       "\n",
       "  Unnamed: 205  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "2          NaN  \n",
       "3          NaN  \n",
       "4          NaN  \n",
       "\n",
       "[5 rows x 569 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df=df.iloc[1:100,:]\n",
    "# df_new=add_hyperlink(df,column_names)\n",
    "df_new = df\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date_format(date_str):\n",
    "    if pd.isna(date_str) or date_str == 'Applicable':\n",
    "        return date_str\n",
    "    try:\n",
    "        # Try parsing as DD-MM-YYYY\n",
    "        return pd.to_datetime(date_str, format='%d-%m-%Y').strftime('%Y-%m-%d')\n",
    "    except:\n",
    "        # If already in YYYY-MM-DD format, return as is\n",
    "        return date_str\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply the conversion to each date column\n",
    "for col in ['Date of sample collection', 'Expiry date', 'Manufacturing date']:\n",
    "    df_new[col] = df_new[col].apply(convert_date_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_change(df2_clean):\n",
    "    # Create masks for different variants\n",
    "    loose_mask = df2_clean['Variant'] == 'Loose'\n",
    "    other_mask = df2_clean['Variant'].isin(['Normal', 'Organic'])\n",
    "    \n",
    "    # Replace values for date columns based on conditions\n",
    "    for col in ['Date of sample collection', 'Expiry date', 'Manufacturing date']:\n",
    "        df2_clean.loc[loose_mask, col] = df2_clean.loc[loose_mask, col].fillna('Not Applicable')\n",
    "        df2_clean.loc[other_mask, col] = df2_clean.loc[other_mask, col].fillna('Not Available')\n",
    "    \n",
    "    return df2_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new1 = date_change(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new1.to_excel('text.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process completed! Check 'commodity_wise_data.xlsx' for results.\n"
     ]
    }
   ],
   "source": [
    "# Makinf different sheet commodity wise\n",
    "# Read the Excel file\n",
    "# df = pd.read_excel('Final_fssai.xlsx')\n",
    "df = df_new1\n",
    "\n",
    "# Get all columns from 'Added Colouring Matter' to 'FSSR Limits Copper Sulphate'\n",
    "all_columns = df.columns.tolist()\n",
    "start_col = 'Commodity'\n",
    "end_col = 'FSSR Limits Copper Sulphate (Copper determined as elemental copper'\n",
    "\n",
    "# Find the indices of start and end columns\n",
    "start_idx = all_columns.index(start_col)\n",
    "end_idx = all_columns.index(end_col)\n",
    "\n",
    "# Create list of columns we want to keep\n",
    "selected_columns = ['Order ID'] + all_columns[start_idx:end_idx+1]\n",
    "\n",
    "# Create a new Excel writer object\n",
    "with pd.ExcelWriter('Final_fssai_commodity_wise.xlsx', engine='openpyxl') as writer:\n",
    "    # Iterate through each unique commodity\n",
    "    for commodity in df['Commodity'].unique():\n",
    "        # Filter data for the current commodity\n",
    "        commodity_data = df[df['Commodity'] == commodity][selected_columns]\n",
    "        \n",
    "        # Clean sheet name (remove special characters that are not allowed in Excel sheet names)\n",
    "        sheet_name = commodity.replace('/', '_').replace('\\\\', '_')[:31]  # Excel has 31 char limit for sheet names\n",
    "        \n",
    "        # Write the filtered data to a new sheet\n",
    "        commodity_data.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "print(\"Process completed! Check 'commodity_wise_data.xlsx' for results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process completed! Check 'commodity_wise_data_reorganized2.xlsx' for results.\n"
     ]
    }
   ],
   "source": [
    "# Reorganize Quality and Safety parameters\n",
    "\n",
    "def get_parameter_columns(df, param_name):\n",
    "    \"\"\"Get all three columns associated with a parameter\"\"\"\n",
    "    return [\n",
    "        col for col in df.columns \n",
    "        if col == param_name \n",
    "        or col == f\"{param_name} Compliance\"\n",
    "        or col.startswith(f\"FSSR Limits {param_name}\")\n",
    "    ]\n",
    "\n",
    "def is_safe_unsafe_param(df, param_name):\n",
    "    \"\"\"Check if parameter has Safe/Unsafe compliance\"\"\"\n",
    "    compliance_col = f\"{param_name} Compliance\"\n",
    "    if compliance_col in df.columns:\n",
    "        unique_values = df[compliance_col].dropna().unique()\n",
    "        return any(val in ['Safe', 'Unsafe'] for val in unique_values)\n",
    "    return False\n",
    "\n",
    "def get_initial_columns(df):\n",
    "    \"\"\"Get all columns from 'Order ID' to 'Count of Passed Residue Analysis'\"\"\"\n",
    "    all_cols = df.columns.tolist()\n",
    "    start_idx = all_cols.index('Order ID')\n",
    "    end_idx = all_cols.index('Count of Passed Residue Analysis')\n",
    "    return all_cols[start_idx:end_idx + 1]\n",
    "\n",
    "def get_base_parameters(df, initial_cols):\n",
    "    \"\"\"Get list of base parameter names (without Compliance or FSSR Limits prefix)\"\"\"\n",
    "    # Get columns that don't have 'Compliance' or 'FSSR Limits' in their names\n",
    "    # and are not in the initial columns list\n",
    "    base_params = [col for col in df.columns \n",
    "                   if 'Compliance' not in col \n",
    "                   and 'FSSR Limits' not in col\n",
    "                   and col not in initial_cols]\n",
    "    return base_params\n",
    "\n",
    "def reorganize_sheet(df):\n",
    "    \"\"\"Reorganize columns based on compliance type\"\"\"\n",
    "    # Get initial columns (from Order ID to Count of Passed Residue Analysis)\n",
    "    initial_columns = get_initial_columns(df)\n",
    "    new_columns = initial_columns\n",
    "    \n",
    "    # Get base parameter names (excluding initial columns)\n",
    "    base_params = get_base_parameters(df, initial_columns)\n",
    "    \n",
    "    # Separate parameters into Safe/Unsafe and Standard/Sub-Standard\n",
    "    safe_unsafe_params = []\n",
    "    standard_substandard_params = []\n",
    "    \n",
    "    for param in base_params:\n",
    "        if is_safe_unsafe_param(df, param):\n",
    "            safe_unsafe_params.append(param)\n",
    "        else:\n",
    "            standard_substandard_params.append(param)\n",
    "    \n",
    "    # Add Safe/Unsafe parameter columns\n",
    "    for param in safe_unsafe_params:\n",
    "        new_columns.extend(get_parameter_columns(df, param))\n",
    "    \n",
    "    # Add Standard/Sub-Standard parameter columns\n",
    "    for param in standard_substandard_params:\n",
    "        new_columns.extend(get_parameter_columns(df, param))\n",
    "    \n",
    "    # Reorder the dataframe columns\n",
    "    return df[new_columns]\n",
    "\n",
    "# # First, let's print out the initial columns to verify\n",
    "# sample_df = pd.read_excel('Final_fssai_commodity_wise.xlsx', sheet_name=0)\n",
    "# initial_cols = get_initial_columns(sample_df)\n",
    "# print(\"Initial columns that will be preserved in order:\")\n",
    "# print(initial_cols)\n",
    "# print(\"\\n\")\n",
    "\n",
    "# Process each sheet and save to new file\n",
    "excel_file = pd.ExcelFile('Final_fssai_commodity_wise.xlsx')\n",
    "\n",
    "with pd.ExcelWriter('Final_fssai_commodity_wise_rearanged.xlsx', engine='openpyxl') as writer:\n",
    "    for sheet_name in excel_file.sheet_names:\n",
    "        # Read the sheet\n",
    "        df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "        \n",
    "        # Reorganize columns\n",
    "        df_reorganized = reorganize_sheet(df)\n",
    "        \n",
    "        # Write to new Excel file\n",
    "        df_reorganized.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "print(\"Process completed! Check 'commodity_wise_data_reorganized2.xlsx' for results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# def get_parameter_columns(df, param_name):\n",
    "#     \"\"\"Get all three columns associated with a parameter\"\"\"\n",
    "#     return [\n",
    "#         col for col in df.columns\n",
    "#         if col == param_name\n",
    "#         or col == f\"{param_name} Compliance\"\n",
    "#         or col.startswith(f\"FSSR Limits {param_name}\")\n",
    "#     ]\n",
    "# def is_column_empty(df, column):\n",
    "#     \"\"\"Check if a column is completely empty (all NaN or empty strings)\"\"\"\n",
    "#     if column not in df.columns:\n",
    "#         return True\n",
    "#     return df[column].isna().all() or (df[column] == '').all()\n",
    "# def is_parameter_complete(df, param_name):\n",
    "#     \"\"\"Check if all three columns for a parameter are present and not empty\"\"\"\n",
    "#     base_col = param_name\n",
    "#     compliance_col = f\"{param_name} Compliance\"\n",
    "#     fssr_col = f\"FSSR Limits {param_name}\"\n",
    "#     # Check if any column is empty\n",
    "#     if is_column_empty(df, base_col) or \\\n",
    "#        is_column_empty(df, compliance_col) or \\\n",
    "#        is_column_empty(df, fssr_col):\n",
    "#         return False\n",
    "#     return True\n",
    "# def get_initial_columns(df):\n",
    "#     \"\"\"Get all columns from 'Order ID' to 'Count of Passed Residue Analysis'\"\"\"\n",
    "#     all_cols = df.columns.tolist()\n",
    "#     start_idx = all_cols.index('Order ID')\n",
    "#     end_idx = all_cols.index('Count of Passed Residue Analysis')\n",
    "#     return all_cols[start_idx:end_idx + 1]\n",
    "# def get_base_parameters(df, initial_cols):\n",
    "#     \"\"\"Get list of base parameter names (without Compliance or FSSR Limits prefix)\"\"\"\n",
    "#     base_params = [col for col in df.columns\n",
    "#                    if 'Compliance' not in col\n",
    "#                    and 'FSSR Limits' not in col\n",
    "#                    and col not in initial_cols]\n",
    "#     return base_params\n",
    "# def is_safe_unsafe_param(df, param_name):\n",
    "#     \"\"\"Check if parameter has Safe/Unsafe compliance\"\"\"\n",
    "#     compliance_col = f\"{param_name} Compliance\"\n",
    "#     if compliance_col in df.columns:\n",
    "#         unique_values = df[compliance_col].dropna().unique()\n",
    "#         return any(val in ['Safe', 'Unsafe'] for val in unique_values)\n",
    "#     return False\n",
    "# def reorganize_sheet(df):\n",
    "#     \"\"\"Reorganize columns and remove incomplete parameter sets\"\"\"\n",
    "#     # Get initial columns\n",
    "#     initial_columns = get_initial_columns(df)\n",
    "#     new_columns = initial_columns.copy()\n",
    "#     # Get base parameter names\n",
    "#     base_params = get_base_parameters(df, initial_columns)\n",
    "#     # Filter out incomplete parameter sets and separate into Safe/Unsafe and Standard/Sub-Standard\n",
    "#     safe_unsafe_params = []\n",
    "#     standard_substandard_params = []\n",
    "#     for param in base_params:\n",
    "#         if is_parameter_complete(df, param):\n",
    "#             if is_safe_unsafe_param(df, param):\n",
    "#                 safe_unsafe_params.append(param)\n",
    "#             else:\n",
    "#                 standard_substandard_params.append(param)\n",
    "#     # Add Safe/Unsafe parameter columns\n",
    "#     for param in safe_unsafe_params:\n",
    "#         new_columns.extend(get_parameter_columns(df, param))\n",
    "#     # Add Standard/Sub-Standard parameter columns\n",
    "#     for param in standard_substandard_params:\n",
    "#         new_columns.extend(get_parameter_columns(df, param))\n",
    "#     # Return dataframe with only the selected columns\n",
    "#     return df[new_columns]\n",
    "# # Process the Excel file\n",
    "# excel_file = pd.ExcelFile('Final_fssai_commodity_wise_rearanged.xlsx.xlsx')\n",
    "# # Create a new Excel writer\n",
    "# with pd.ExcelWriter('Final_fssai_commodity_wise_rearanged_drop.xlsx', engine='openpyxl') as writer:\n",
    "#     for sheet_name in excel_file.sheet_names:\n",
    "#         print(f\"\\nProcessing sheet: {sheet_name}\")\n",
    "#         # Read the sheet\n",
    "#         df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "#         original_cols = len(df.columns)\n",
    "#         # Reorganize and clean columns\n",
    "#         df_cleaned = reorganize_sheet(df)\n",
    "#         final_cols = len(df_cleaned.columns)\n",
    "#         print(f\"Columns removed: {original_cols - final_cols}\")\n",
    "#         # Write to new Excel file\n",
    "#         df_cleaned.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "# print(\"\\nProcess completed! Check 'commodity_wise_data_complete_params.xlsx' for results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing sheet: Coriander Whole\n",
      "Columns removed: 189\n",
      "\n",
      "Processing sheet: Turmeric Powder\n",
      "Columns removed: 344\n",
      "\n",
      "Processing sheet: Chilli Powder\n",
      "Columns removed: 135\n",
      "\n",
      "Processing sheet: Cumin Powder\n",
      "Columns removed: 323\n",
      "\n",
      "Processing sheet: Cinnamon (Dalchini) Whole\n",
      "Columns removed: 366\n",
      "\n",
      "Processing sheet: Coriander Powder\n",
      "Columns removed: 381\n",
      "\n",
      "Processing sheet: Black Pepper Powder\n",
      "Columns removed: 379\n",
      "Not found\n",
      "\n",
      "Processing sheet: Cumin Whole\n",
      "Columns removed: 330\n",
      "Not found\n",
      "\n",
      "Processing sheet: Cardamon Whole\n",
      "Columns removed: 351\n",
      "\n",
      "Processing sheet: Turmeric Whole\n",
      "Columns removed: 369\n",
      "\n",
      "Processing sheet: Black Pepper Whole\n",
      "Columns removed: 104\n",
      "\n",
      "Processing sheet: Chilli whole\n",
      "Columns removed: 198\n",
      "\n",
      "Processing sheet: Cardamom powder\n",
      "Columns removed: 357\n",
      "\n",
      "Process completed! Check 'commodity_wise_data_cleaned_final2.xlsx' for results.\n"
     ]
    }
   ],
   "source": [
    "#Drop empty cols\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_parameter_columns(df, param_name):\n",
    "    \"\"\"Get all three columns associated with a parameter\"\"\"\n",
    "    return [\n",
    "        col for col in df.columns \n",
    "        if col == param_name \n",
    "        or col == f\"{param_name} Compliance\"\n",
    "        or col.startswith(f\"FSSR Limits {param_name}\")\n",
    "    ]\n",
    "\n",
    "def is_param_empty(df, param_name):\n",
    "    \"\"\"Check if the parameter column is completely empty\"\"\"\n",
    "    if param_name not in df.columns:\n",
    "        return True\n",
    "    return df[param_name].isna().all() or (df[param_name] == '').all()\n",
    "\n",
    "def get_initial_columns(df):\n",
    "    \"\"\"Get all columns from 'Order ID' to 'Count of Passed Residue Analysis'\"\"\"\n",
    "    all_cols = df.columns.tolist()\n",
    "    start_idx = all_cols.index('Order ID')\n",
    "    end_idx = all_cols.index('Count of Passed Residue Analysis')\n",
    "    return all_cols[start_idx:end_idx + 1]\n",
    "\n",
    "def get_base_parameters(df, initial_cols):\n",
    "    \"\"\"Get list of base parameter names (without Compliance or FSSR Limits prefix)\"\"\"\n",
    "    base_params = [col for col in df.columns \n",
    "                   if 'Compliance' not in col \n",
    "                   and 'FSSR Limits' not in col\n",
    "                   and col not in initial_cols]\n",
    "    return base_params\n",
    "\n",
    "def clean_sheet(df):\n",
    "    \"\"\"Remove parameter groups where parameter column is empty\"\"\"\n",
    "    # Get initial columns\n",
    "    initial_columns = get_initial_columns(df)\n",
    "    columns_to_keep = initial_columns.copy()\n",
    "    \n",
    "    # Get base parameter names\n",
    "    base_params = get_base_parameters(df, initial_columns)\n",
    "    \n",
    "    # For each parameter, check if its parameter column is empty\n",
    "    for param in base_params:\n",
    "        if not is_param_empty(df, param):\n",
    "            # If parameter column is not empty, keep all three associated columns\n",
    "            columns_to_keep.extend(get_parameter_columns(df, param))\n",
    "    \n",
    "    # Return dataframe with only the selected columns\n",
    "    return df[columns_to_keep]\n",
    "\n",
    "# Read the existing Excel file\n",
    "excel_file = pd.ExcelFile('Final_fssai_commodity_wise_rearanged.xlsx')\n",
    "\n",
    "# Process each sheet and save to new file\n",
    "with pd.ExcelWriter('commodity_wise_data_cleaned_final2.xlsx', engine='openpyxl') as writer:\n",
    "    for sheet_name in excel_file.sheet_names:\n",
    "        print(f\"\\nProcessing sheet: {sheet_name}\")\n",
    "        \n",
    "        # Read the sheet\n",
    "        df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "        original_cols = len(df.columns)\n",
    "        \n",
    "        # Clean the sheet by removing parameter groups with empty parameter columns\n",
    "        df_cleaned = clean_sheet(df)\n",
    "        final_cols = len(df_cleaned.columns)\n",
    "        \n",
    "        print(f\"Columns removed: {original_cols - final_cols}\")\n",
    "\n",
    "        df_cleaned = add_hyperlink(df_cleaned,column_names)\n",
    "        try:\n",
    "            # print('len ', len(df_cleaned['FSSR Limits Mouldy Berries, %']))\n",
    "            colnames = ['FSSR Limits Mouldy Berries, %', 'FSSR Limits Mouldy seeds,%', 'FSSR Limits Mouldy Berries, %.1','FSSR Limits Mouldy seeds,%.1']\n",
    "\n",
    "            for colname in colnames:\n",
    "                df_cleaned.shape[0] - df_cleaned[colname].isna().sum() == 0\n",
    "                df_cleaned.drop(columns=colname, inplace=True)\n",
    "\n",
    "            df_cleaned = df_cleaned[[col for col in df_cleaned.columns if not col.endswith('_y')]]\n",
    "            df_cleaned = df_cleaned[[col for col in df_cleaned.columns if not col.endswith('.1')]]\n",
    "        except:\n",
    "            print('Not found')\n",
    "\n",
    "        # df_cleaned = df_cleaned.drop(columns=['FSSR Limits Mouldy Berries, %', 'FSSR Limits Mouldy seeds,%', 'FSSR Limits Mouldy Berries, %.1','FSSR Limits Mouldy seeds,%.1'])\n",
    "        \n",
    "        # Write to new Excel file\n",
    "        df_cleaned.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "print(\"\\nProcess completed! Check 'commodity_wise_data_cleaned_final2.xlsx' for results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colnames = ['FSSR Limits Mouldy Berries, %', 'FSSR Limits Mouldy seeds,%', 'FSSR Limits Mouldy Berries, %.1','FSSR Limits Mouldy seeds,%.1']\n",
    "\n",
    "# for colname in colnames:\n",
    "#     df_cleaned.shape[0] - df_cleaned[colname].isna().sum() == 0\n",
    "#     df_cleaned.drop(columns=colname, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'FSSR Limits Mouldy Berries, %', 'FSSR Limits Mouldy seeds,%', 'FSSR Limits Mouldy Berries, %.1','FSSR Limits Mouldy seeds,%.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# def add_hyperlink(df, column_names):\n",
    "#     for column in column_names:\n",
    "#         df[column] = df[column].apply(lambda x: f'=HYPERLINK(\"{x}\", \"{x}\")' if pd.notna(x) else x)\n",
    "#     return df\n",
    "\n",
    "# def date_change(df2_clean):\n",
    "#     # Create masks for different variants\n",
    "#     loose_mask = df2_clean['Variant'] == 'Loose'\n",
    "#     other_mask = df2_clean['Variant'].isin(['Normal', 'Organic'])\n",
    "    \n",
    "#     # Replace values for date columns based on conditions\n",
    "#     for col in ['Date of sample collection', 'Expiry date', 'Manufacturing date']:\n",
    "#         df2_clean.loc[loose_mask, col] = df2_clean.loc[loose_mask, col].fillna('Not Applicable')\n",
    "#         df2_clean.loc[other_mask, col] = df2_clean.loc[other_mask, col].fillna('Not Available')\n",
    "    \n",
    "#     return df2_clean\n",
    "\n",
    "# # Columns to add hyperlinks\n",
    "# column_names = [\n",
    "#     'Report Link', \n",
    "#     'Picture of the shop', \n",
    "#     'General Packaging Link', \n",
    "#     'Manufacturing Details Photo Link'\n",
    "# ]\n",
    "\n",
    "# # Read Excel file\n",
    "# df = pd.read_excel('fssai.xlsx', sheet_name='MASTER DATA')\n",
    "\n",
    "# # Process the dataframe\n",
    "# df_new = add_hyperlink(df, column_names)\n",
    "# df_new = date_change(df_new)\n",
    "\n",
    "# # Save the processed dataframe to a new Excel file\n",
    "# df_new.to_excel('processed_fssai_data.xlsx', index=False)\n",
    "\n",
    "# # Optional: Print confirmation message\n",
    "# print(\"Processed data saved to 'processed_fssai_data.xlsx'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file saved as output.xlsx with conditional formatting.\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# from openpyxl import load_workbook\n",
    "# from openpyxl.styles import PatternFill\n",
    "\n",
    "# # Sample DataFrame\n",
    "# data = {'safe/unsafe': ['safe', 'unsafe', 'safe', 'unsafe', 'safe']}\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# # Export DataFrame to Excel\n",
    "# excel_path = \"output.xlsx\"\n",
    "# df.to_excel(excel_path, index=False)\n",
    "\n",
    "# # Load the Excel file\n",
    "# wb = load_workbook(excel_path)\n",
    "# ws = wb.active\n",
    "\n",
    "# # Define colors\n",
    "# green_fill = PatternFill(start_color=\"00FF00\", end_color=\"00FF00\", fill_type=\"solid\")\n",
    "# red_fill = PatternFill(start_color=\"FF0000\", end_color=\"FF0000\", fill_type=\"solid\")\n",
    "\n",
    "# # Apply conditional formatting (starting from row 2 to skip the header)\n",
    "# for row in ws.iter_rows(min_row=2, max_row=len(df) + 1, min_col=1, max_col=1):\n",
    "#     for cell in row:\n",
    "#         if cell.value == \"safe\":\n",
    "#             cell.fill = green_fill\n",
    "#         elif cell.value == \"unsafe\":\n",
    "#             cell.fill = red_fill\n",
    "\n",
    "# # Save the modified Excel file\n",
    "# wb.save(excel_path)\n",
    "\n",
    "# print(f\"Excel file saved as {excel_path} with conditional formatting.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
